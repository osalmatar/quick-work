{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and clean multiple CSV files and process all unique tickers\n",
    "def load_and_merge_csv():\n",
    "    # Load datasets\n",
    "    A = pd.read_csv('ATR.csv')\n",
    "    B = pd.read_csv('D_EW_B.csv')\n",
    "    C = pd.read_csv('D_EW_S.csv')\n",
    "    D = pd.read_csv('HHLL.csv')\n",
    "    E = pd.read_csv('LR_Explore.csv')\n",
    "    F = pd.read_csv('Pattern_Revv.csv')\n",
    "    G = pd.read_csv('SCTR_Trial.csv')\n",
    "    H = pd.read_csv('ZigZag.csv')\n",
    "\n",
    "    # Process and clean datasets\n",
    "    for df in [A, B, C, D, E, F, G, H]:\n",
    "        df['Date/Time'] = pd.to_datetime(df['Date/Time'], errors='coerce')\n",
    "\n",
    "    # Rename columns\n",
    "    A.rename(columns={'Buy': 'Buy_ATR'}, inplace=True)\n",
    "    A['Buy_ATR'] = A['Buy_ATR'].replace(1, 'ATR')\n",
    "\n",
    "    B.rename(columns={'Mega Buy by Larger Wave ': 'Buy_EW'}, inplace=True)\n",
    "    B['Buy_EW'] = 'EW'\n",
    "\n",
    "    E.rename(columns={'myBuy ': 'Buy_LR'}, inplace=True)\n",
    "    E['Buy_LR'] = E['Buy_LR'].replace(1, 'LR')\n",
    "\n",
    "    F.rename(columns={'BullBreak': 'Buy_BB'}, inplace=True)\n",
    "    F['Buy_BB'] = F['Buy_BB'].replace('bullish Breakout', 'BB')\n",
    "\n",
    "    H.rename(columns={'Buy': 'Buy_ZZ'}, inplace=True)\n",
    "    H['Buy_ZZ'] = H['Buy_ZZ'].replace(1, 'ZZ')\n",
    "\n",
    "    # Map patterns\n",
    "    pattern_mapping = {\n",
    "        'Up Channel': '_UC',\n",
    "        'Wedge': '_W',\n",
    "        'Down Channel': '_DC',\n",
    "        'Broadening Wedge': '_BW',\n",
    "        'Ascending Triangle': '_AT',\n",
    "        'Decending Triangle': '_DT'\n",
    "    }\n",
    "    F['Pattern'] = F['Pattern'].replace(pattern_mapping)\n",
    "\n",
    "    # Merge tables and create final table\n",
    "    tables = [A, B, C, D, E, F, G, H]\n",
    "    final_table = pd.DataFrame()\n",
    "\n",
    "    for table in tables:\n",
    "        if 'Ticker' in table.columns:\n",
    "            temp = pd.melt(table, id_vars=['Date/Time', 'Ticker'], var_name='Field', value_name='Value')\n",
    "        else:\n",
    "            temp = pd.melt(table, id_vars=['Date/Time'], var_name='Field', value_name='Value')\n",
    "        final_table = pd.concat([final_table, temp], ignore_index=True)\n",
    "\n",
    "    final_table = final_table.pivot_table(index=['Date/Time', 'Ticker'], columns='Field', values='Value', aggfunc='first').reset_index()\n",
    "\n",
    "    # Fill NaNs and concatenate columns\n",
    "    final_table[['Buy_ATR', 'Buy_EW', 'Buy_BB', 'Buy_LR', 'Pattern']] = final_table[['Buy_ATR', 'Buy_EW', 'Buy_BB', 'Buy_LR', 'Pattern']].fillna('')\n",
    "\n",
    "    def concatenate_non_nan(row):\n",
    "        values = [row['Buy_ATR'], row['Buy_EW'], row['Buy_BB'], row['Buy_LR'], row['Pattern']]\n",
    "        non_nan_values = [str(v) for v in values if pd.notna(v)]\n",
    "        return ''.join(non_nan_values)\n",
    "\n",
    "    final_table['Buy Strat'] = final_table.apply(concatenate_non_nan, axis=1)\n",
    "\n",
    "    # Exclude unnecessary patterns and rows with NaN Buy Strat\n",
    "    exclude_values = ['_UC', '_W', '_DC', '_BW', '_AT', '_DT']\n",
    "    final_table = final_table[~final_table['Buy Strat'].isin(exclude_values)]\n",
    "    final_table = final_table[final_table['Buy Strat'].notna() & (final_table['Buy Strat'] != '')]\n",
    "\n",
    "    # Filter to include only the rows for the most recent date\n",
    "    max_date = final_table['Date/Time'].max()\n",
    "    final_table = final_table[final_table['Date/Time'] == max_date]\n",
    "\n",
    "    # Rename and keep only the necessary columns\n",
    "    final_table = final_table[['Date/Time', 'Ticker', 'Buy Strat', 'll']]\n",
    "    final_table = final_table.rename(columns={'Date/Time': 'Buy Date'})\n",
    "    final_table.to_csv('final_table.csv', index=False)\n",
    "\n",
    "    return final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch CMP from PostgreSQL yfinance table and ew_conv table, then merge with the tickers\n",
    "def fetch_and_merge_from_postgresql(final_table):\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Fetch CMP and CMP_Date from yfinance_data\n",
    "    cmp_query = 'SELECT \"Ticker\", \"CMP\", \"CMP_Date\" FROM yfinance_data'\n",
    "    cmp_df = pd.read_sql(cmp_query, conn)\n",
    "\n",
    "    # Merge CMP data with final_table\n",
    "    merged_table = pd.merge(final_table, cmp_df, on='Ticker', how='left')\n",
    "\n",
    "    # Fetch ew_conv_table data with Target field\n",
    "    ew_conv_query = '''\n",
    "    SELECT \"Ticker\", \"Last Alert Date\", \"Target\", \"Call\", \"Status\" \n",
    "    FROM ew_conv_table\n",
    "    '''\n",
    "    ew_conv_df = pd.read_sql(ew_conv_query, conn)\n",
    "\n",
    "    # Merge ew_conv data with the merged_table based on Ticker\n",
    "    merged_table = pd.merge(merged_table, ew_conv_df, on='Ticker', how='left')\n",
    "\n",
    "    # Fetch Close from merged_tickers\n",
    "    merged_tickers_query = 'SELECT \"Ticker\", \"Date/Time\", \"Close\" FROM merged_tickers'\n",
    "    merged_tickers_df = pd.read_sql(merged_tickers_query, conn)\n",
    "    merged_tickers_df.rename(columns={'Date/Time':'Buy Date'}, inplace=True)\n",
    "    merged_tickers_df['Buy Date'] = pd.to_datetime(merged_tickers_df['Buy Date'], errors='coerce')\n",
    "\n",
    "    # Merge merged_tickers for Close (Buy Price) with merged_table based on ticker and date/time\n",
    "    merged_table = pd.merge(merged_table, merged_tickers_df, on = ['Ticker', 'Buy Date'], how = 'left') # left to verify, but inner SHOULD give same results\n",
    "\n",
    "    # Assign 'India' to the Broker field after merging the tables\n",
    "    merged_table['Broker'] = 'India'\n",
    "\n",
    "    # Ensure Date fields are in the correct date format\n",
    "    merged_table['CMP_Date'] = pd.to_datetime(merged_table['CMP_Date'], errors='coerce').dt.date\n",
    "    merged_table['Last Alert Date'] = pd.to_datetime(merged_table['Last Alert Date'], errors='coerce').dt.date\n",
    "    merged_table['Buy Date'] = pd.to_datetime(merged_table['Buy Date'], errors='coerce').dt.date\n",
    "\n",
    "    # Handle missing values in the date columns\n",
    "    placeholder_date = datetime(2000, 1, 1).date()\n",
    "    merged_table['CMP_Date'] = merged_table['CMP_Date'].fillna(placeholder_date)\n",
    "    merged_table['Last Alert Date'] = merged_table['Last Alert Date'].fillna(placeholder_date)\n",
    "    merged_table['Buy Date'] = merged_table['Buy Date'].fillna(placeholder_date)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Calculate quantity safely and avoiding division by zero\n",
    "    # Define a safe calculation function\n",
    "    def calculate_quantity(row):\n",
    "        if pd.notnull(row['Close']) and pd.notnull(row['ll']) and row['Close'] > row['ll']:\n",
    "            result = 1000 / (row['Close'] - row['ll'])\n",
    "            return round(result, 2)  # Use round() instead of .round()\n",
    "        return None  # Return None if the conditions are not met\n",
    "    # Apply the safe function to calculate Quantity\n",
    "    merged_table['Quantity'] = merged_table.apply(calculate_quantity, axis=1)\n",
    "\n",
    "    merged_table['CMP'] = merged_table['CMP'].round(2)\n",
    "    merged_table['ll'] = merged_table['ll'].round(2)\n",
    "    merged_table['Close'] = merged_table['Close'].round(2)\n",
    "    merged_table['Target'] = merged_table['Target'].round(2)\n",
    "\n",
    "    # Apply logic for Call and Status adjustments\n",
    "    def adjust_call_status(row):\n",
    "        # Check if CMP < ll to hit ISL\n",
    "        if pd.notnull(row['CMP']) and pd.notnull(row['ll']) and row['CMP'] < row['ll']:\n",
    "            return 'Short', 'Closed', 'ISL Hit', round(row['ll'], 2)\n",
    "        # Check if CMP <= Target to hit Target from ew_conv_table\n",
    "        elif pd.notnull(row['CMP']) and pd.notnull(row['Target']) and row['CMP'] <= row['Target']:\n",
    "            return 'Short', 'Closed', 'Target Hit', round(row['Target'], 2)\n",
    "        # If conditions are not met, return original Call, Status, Hit Type, and Sell Price as None\n",
    "        return row['Call'], row['Status'], None, None\n",
    "\n",
    "    merged_table[['Call', 'Status', 'Hit Type', 'Sell Price']] = merged_table.apply(\n",
    "        lambda row: adjust_call_status(row), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "    merged_table['Sell Price'] = merged_table['Sell Price'].round(2)\n",
    "\n",
    "    return merged_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final data to CSV and upload to PostgreSQL\n",
    "def save_and_upload(merged_table):\n",
    "    # Ensure correct data types\n",
    "    merged_table['ll'] = pd.to_numeric(merged_table['ll'], errors='coerce')\n",
    "    merged_table['Close'] = pd.to_numeric(merged_table['Close'], errors='coerce')\n",
    "    merged_table['Target'] = pd.to_numeric(merged_table['Target'], errors='coerce')\n",
    "    merged_table['CMP'] = pd.to_numeric(merged_table['CMP'], errors='coerce')\n",
    "    merged_table['Quantity'] = pd.to_numeric(merged_table['Quantity'], errors='coerce')\n",
    "    merged_table['Sell Price'] = pd.to_numeric(merged_table['Sell Price'], errors='coerce')\n",
    "    merged_table['Buy Date'] = pd.to_datetime(merged_table['Buy Date']).dt.date\n",
    "    merged_table['Last Alert Date'] = pd.to_datetime(merged_table['Last Alert Date']).dt.date\n",
    "    merged_table['CMP_Date'] = pd.to_datetime(merged_table['CMP_Date']).dt.date\n",
    "\n",
    "    # Ensure column order matches the SQL query\n",
    "    merged_table = merged_table[['Buy Date', 'Ticker', 'Buy Strat', 'Last Alert Date', 'll', 'Close', \n",
    "                               'Target', 'Call', 'Status', 'CMP', 'CMP_Date', 'Quantity', 'Hit Type', \n",
    "                               'Sell Price', 'Broker']]\n",
    "\n",
    "    # Save the final table to CSV\n",
    "    merged_table.to_csv('final_trade_journal.csv', index=False)\n",
    "\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create Trade Journal table if it doesn't exist\n",
    "    create_table_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS trade_journal (\n",
    "        \"Buy Date\" DATE,\n",
    "        \"Ticker\" TEXT,\n",
    "        \"Buy Strat\" TEXT,\n",
    "        \"Last Alert Date\" DATE,\n",
    "        \"ll\" FLOAT,\n",
    "        \"Close\" FLOAT,\n",
    "        \"Target\" FLOAT,\n",
    "        \"Call\" TEXT,\n",
    "        \"Status\" TEXT,\n",
    "        \"CMP\" FLOAT,\n",
    "        \"CMP_Date\" DATE,\n",
    "        \"Quantity\" FLOAT,\n",
    "        \"Hit Type\" TEXT,\n",
    "        \"Sell Price\" FLOAT, \n",
    "        \"Broker\" TEXT,\n",
    "        PRIMARY KEY (\"Ticker\", \"Buy Date\")\n",
    "    );\n",
    "    '''\n",
    "    cur.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert or update data into PostgreSQL\n",
    "    for _, row in merged_table.iterrows():\n",
    "        # Insert or update for composite key (\"Buy Date\", \"Ticker\")\n",
    "        insert_query_composite = '''\n",
    "        INSERT INTO trade_journal (\"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \"Close\", \"Target\", \n",
    "        \"Call\", \"Status\", \"Quantity\", \"Hit Type\", \"Sell Price\", \"Broker\") \n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (\"Buy Date\", \"Ticker\")\n",
    "        DO UPDATE SET \n",
    "            \"Buy Strat\" = EXCLUDED.\"Buy Strat\",\n",
    "            \"Last Alert Date\" = EXCLUDED.\"Last Alert Date\",\n",
    "            \"ll\" = EXCLUDED.\"ll\",\n",
    "            \"Close\" = EXCLUDED.\"Close\",\n",
    "            \"Target\" = EXCLUDED.\"Target\",\n",
    "            \"Call\" = EXCLUDED.\"Call\",\n",
    "            \"Status\" = EXCLUDED.\"Status\",\n",
    "            \"Quantity\" = EXCLUDED.\"Quantity\",\n",
    "            \"Hit Type\" = EXCLUDED.\"Hit Type\",\n",
    "            \"Sell Price\" = EXCLUDED.\"Sell Price\",\n",
    "            \"Broker\" = EXCLUDED.\"Broker\";\n",
    "        '''\n",
    "        cur.execute(insert_query_composite, (\n",
    "            row[\"Buy Date\"], row[\"Ticker\"], row[\"Buy Strat\"], row[\"Last Alert Date\"], row[\"ll\"], row[\"Close\"], \n",
    "            row[\"Target\"], row[\"Call\"], row[\"Status\"], row[\"Quantity\"], \n",
    "            row[\"Hit Type\"], row[\"Sell Price\"], row[\"Broker\"]\n",
    "    ))\n",
    "\n",
    "    # Insert or update for \"CMP\" and \"CMP_Date\" based on \"Ticker\" only\n",
    "    update_query_cmp = '''\n",
    "    UPDATE trade_journal \n",
    "    SET\n",
    "        \"CMP\" = y.\"CMP\",\n",
    "        \"CMP_Date\" = y.\"CMP_Date\"\n",
    "    FROM yfinance_data y\n",
    "    WHERE trade_journal.\"Ticker\" = y.\"Ticker\"\n",
    "    '''\n",
    "    cur.execute(update_query_cmp)\n",
    "    \n",
    "    # Apply logic for Call and Status adjustments\n",
    "    def adjust_call_status(row):\n",
    "        # Check if CMP < ll to hit ISL\n",
    "        if pd.notnull(row['CMP']) and pd.notnull(row['ll']) and row['CMP'] < row['ll']:\n",
    "            return 'Short', 'Closed', 'ISL Hit', round(row['ll'], 2)\n",
    "        # Check if CMP <= Target to hit Target from ew_conv_table\n",
    "        elif pd.notnull(row['CMP']) and pd.notnull(row['Target']) and row['CMP'] <= row['Target']:\n",
    "            return 'Short', 'Closed', 'Target Hit', round(row['Target'], 2)\n",
    "        # If conditions are not met, return original Call, Status, Hit Type, and Sell Price as None\n",
    "        return row['Call'], row['Status'], None, None\n",
    "\n",
    "    merged_table[['Call', 'Status', 'Hit Type', 'Sell Price']] = merged_table.apply(\n",
    "        lambda row: adjust_call_status(row), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transfer specific records to a new table\n",
    "def transfer_closed():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create the new table for archived records if it doesn't exist\n",
    "    create_archive_table_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS archived_trades (\n",
    "        \"Buy Date\" DATE,\n",
    "        \"Ticker\" TEXT,\n",
    "        \"Buy Strat\" TEXT,\n",
    "        \"Last Alert Date\" DATE,\n",
    "        \"ll\" FLOAT,\n",
    "        \"Close\" FLOAT,\n",
    "        \"Target\" FLOAT,\n",
    "        \"Call\" TEXT,\n",
    "        \"Status\" TEXT,\n",
    "        \"CMP\" FLOAT,\n",
    "        \"CMP_Date\" DATE,\n",
    "        \"Quantity\" FLOAT,\n",
    "        \"Hit Type\" TEXT,\n",
    "        \"Sell Price\" FLOAT,\n",
    "        \"Broker\" TEXT,\n",
    "        PRIMARY KEY (\"Ticker\", \"Buy Date\")\n",
    "    );\n",
    "    '''\n",
    "    cur.execute(create_archive_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Step 1: Retrieve Close for each Ticker before deletion\n",
    "    buy_price_query = '''\n",
    "    SELECT \"Ticker\", \"Close\" \n",
    "    FROM trade_journal\n",
    "    WHERE \"Close\" IS NOT NULL\n",
    "    '''\n",
    "    buy_price_df = pd.read_sql(buy_price_query, conn)\n",
    "\n",
    "    # Convert buy_price_df to dictionary for easy lookup\n",
    "    close_dict = buy_price_df.set_index('Ticker')['Close'].to_dict()\n",
    "\n",
    "    # Step 2: Fetch closed records to transfer\n",
    "    closed_records_query = '''\n",
    "    SELECT \n",
    "        \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \n",
    "        \"Close\", \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\", \n",
    "        \"Quantity\", \"Hit Type\", \"Sell Price\", \"Broker\"\n",
    "    FROM trade_journal\n",
    "    WHERE \"Status\" = 'Closed'\n",
    "    '''\n",
    "    closed_records_df = pd.read_sql(closed_records_query, conn)\n",
    "\n",
    "    # Step 3: Fill missing Close values in closed_records_df using buy_price_dict\n",
    "    closed_records_df['Close'] = closed_records_df.apply(\n",
    "        lambda row: close_dict.get(row['Ticker'], row['Close']) \n",
    "        if pd.isna(row['Close']) else row['Close'], axis=1\n",
    "    )\n",
    "\n",
    "    # Step 4: Insert updated closed records into archived_trades\n",
    "    for _, row in closed_records_df.iterrows():\n",
    "        insert_into_archived_query = '''\n",
    "        INSERT INTO archived_trades (\n",
    "            \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \"Close\", \n",
    "            \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\", \"Quantity\", \"Hit Type\", \n",
    "            \"Sell Price\", \"Broker\")\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (\"Buy Date\", \"Ticker\")\n",
    "        DO NOTHING;\n",
    "        '''\n",
    "        cur.execute(insert_into_archived_query, tuple(row))\n",
    "    conn.commit()\n",
    "\n",
    "    # Step 5: Delete the Closed records after transferring to archived_trades table\n",
    "    delete_query = '''\n",
    "    DELETE FROM trade_journal\n",
    "    WHERE \"Status\" = 'Closed';\n",
    "    '''\n",
    "    cur.execute(delete_query)\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transfer specific records where the bought and sold tickers are on the same day\n",
    "def transfer_reversed():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create the new table for reversed records if it doesn't exist\n",
    "    create_reversed_table_query = '''\n",
    "    CREATE TABLE IF NOT EXISTS reversed_table (\n",
    "        \"Buy Date\" DATE,\n",
    "        \"Ticker\" TEXT,\n",
    "        \"Buy Strat\" TEXT,\n",
    "        \"Last Alert Date\" DATE,\n",
    "        \"ll\" FLOAT,\n",
    "        \"Close\" FLOAT,\n",
    "        \"Target\" FLOAT,\n",
    "        \"Call\" TEXT,\n",
    "        \"Status\" TEXT,\n",
    "        \"CMP\" FLOAT,\n",
    "        \"CMP_Date\" DATE,\n",
    "        \"Quantity\" FLOAT,\n",
    "        \"Hit Type\" TEXT,\n",
    "        \"Sell Price\" FLOAT,\n",
    "        \"Broker\" TEXT,\n",
    "        PRIMARY KEY (\"Ticker\", \"Buy Date\")\n",
    "    );\n",
    "    '''\n",
    "    cur.execute(create_reversed_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Fetch all records from trade_journal for processing\n",
    "    fetch_records_query = '''\n",
    "    SELECT \n",
    "        \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \n",
    "        \"Close\", \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\",\n",
    "        \"Quantity\", \"Hit Type\", \"Sell Price\", \"Broker\"\n",
    "    FROM trade_journal\n",
    "    WHERE \"Buy Date\" = \"CMP_Date\" AND \"Status\" = 'Closed';\n",
    "    '''\n",
    "    records_df = pd.read_sql(fetch_records_query, conn)\n",
    "\n",
    "    records_df['Status'] = 'Open'\n",
    "    records_df['Call'] = 'Long'\n",
    "\n",
    "    # Recalculate Risk Amount and Target for all records\n",
    "    records_df['Risk Amount'] = records_df['Close'] - records_df['ll']\n",
    "    records_df['Target'] = (2 * records_df['Risk Amount']) + records_df['Close']\n",
    "    records_df['Target'] = records_df['Target'].round(2)\n",
    "\n",
    "    # Insert updated records into reversed_table\n",
    "    for _, row in records_df.iterrows():\n",
    "        insert_into_reversed_query = '''\n",
    "        INSERT INTO reversed_table (\n",
    "            \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \"Close\", \n",
    "            \"Target\", \"Call\", \"Status\", \"Quantity\", \"Hit Type\", \"CMP\", \"CMP_Date\",\n",
    "            \"Sell Price\", \"Broker\")\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (\"Buy Date\", \"Ticker\")\n",
    "        DO NOTHING;\n",
    "        '''\n",
    "        cur.execute(insert_into_reversed_query, (\n",
    "            row[\"Buy Date\"], row[\"Ticker\"], row[\"Buy Strat\"], row[\"Last Alert Date\"],\n",
    "            row[\"ll\"], row[\"Close\"], row[\"Target\"], row[\"Call\"], row[\"Status\"],\n",
    "            row[\"Quantity\"], row[\"Hit Type\"], row['CMP'], row['CMP_Date'], row[\"Sell Price\"], row[\"Broker\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "\n",
    "    # Function to adjust Call, Status, Hit Type, and Sell Price using the updated Target\n",
    "    def adjust_call_status(row):\n",
    "        # Check if CMP < ll to hit ISL\n",
    "        if pd.notnull(row['CMP']) and pd.notnull(row['ll']) and row['CMP'] < row['ll']:\n",
    "            return 'Short', 'Closed', 'ISL Hit', round(row['ll'], 2)\n",
    "        # Check if CMP <= Target to hit the adjusted Target\n",
    "        elif pd.notnull(row['CMP']) and pd.notnull(row['Target']) and row['CMP'] >= row['Target']:\n",
    "            return 'Short', 'Closed', 'Target Hit', round(row['Target'], 2)\n",
    "        # If conditions are not met, return original Call, Status, Hit Type, and Sell Price as None\n",
    "        return row['Call'], row['Status'], None, None\n",
    "\n",
    "    # Apply the logic to update the DataFrame\n",
    "    records_df[['Call', 'Status', 'Hit Type', 'Sell Price']] = records_df.apply(\n",
    "        lambda row: adjust_call_status(row), axis=1, result_type='expand'\n",
    "    )\n",
    "\n",
    "    # Round Sell Price to 2 decimal places\n",
    "    records_df['Sell Price'] = records_df['Sell Price'].round(2)\n",
    "\n",
    "    # Update reversed_table with adjusted values\n",
    "    for _, row in records_df.iterrows():\n",
    "        update_adjusted_values_query = '''\n",
    "        UPDATE reversed_table\n",
    "        SET \n",
    "            \"Call\" = %s,\n",
    "            \"Status\" = %s,\n",
    "            \"Hit Type\" = %s,\n",
    "            \"Sell Price\" = %s\n",
    "        WHERE \"Buy Date\" = %s AND \"Ticker\" = %s;\n",
    "        '''\n",
    "        cur.execute(update_adjusted_values_query, (\n",
    "            row[\"Call\"], row[\"Status\"], row[\"Hit Type\"], row[\"Sell Price\"],\n",
    "            row[\"Buy Date\"], row[\"Ticker\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "\n",
    "    # Separate Closed records to transfer to archived_trades\n",
    "    to_archive = records_df[records_df[\"Status\"] == \"Closed\"]\n",
    "\n",
    "    # Insert Closed records into archived_trades\n",
    "    for _, row in to_archive.iterrows():\n",
    "        insert_into_archive_query = '''\n",
    "        INSERT INTO archived_trades (\n",
    "            \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \"Close\", \n",
    "            \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\", \"Quantity\", \"Hit Type\", \n",
    "            \"Sell Price\", \"Broker\")\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (\"Buy Date\", \"Ticker\")\n",
    "        DO NOTHING;\n",
    "        '''\n",
    "        cur.execute(insert_into_archive_query, (\n",
    "            row[\"Buy Date\"], row[\"Ticker\"], row[\"Buy Strat\"], row[\"Last Alert Date\"],\n",
    "            row[\"ll\"], row[\"Close\"], row[\"Target\"], row[\"Call\"], row[\"Status\"],\n",
    "            row[\"CMP\"], row[\"CMP_Date\"], row[\"Quantity\"], row[\"Hit Type\"],\n",
    "            row[\"Sell Price\"], row[\"Broker\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "\n",
    "    # Delete the Closed records from trade_journal after archiving\n",
    "    delete_query = '''\n",
    "    DELETE FROM trade_journal\n",
    "    WHERE \"Buy Date\" = \"CMP_Date\" AND \"Status\" = 'Closed';\n",
    "    '''\n",
    "    cur.execute(delete_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Delete the Closed records from reversed_table after archiving\n",
    "    delete_query2 = '''\n",
    "    DELETE FROM reversed_table\n",
    "    WHERE \"Status\" = 'Closed';\n",
    "    '''\n",
    "    cur.execute(delete_query2)\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new function that imports data from ew conv, trade journal, and reversed table, and outputs 2 excel files (buy and sell)\n",
    "# Function finds the tickers in the last 2 days alert from ew conv that also exist in TJ or reversed table.\n",
    "def fetch_and_export_to_excel():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Fetch Ticker from Trade_Journal\n",
    "    tj_ticker_query = ''' SELECT \"Ticker\", \"Broker\" FROM trade_journal'''\n",
    "    tj_ticker_df = pd.read_sql(tj_ticker_query, conn)\n",
    "\n",
    "    # Fetch Tickers from reversed_table\n",
    "    reversed_query = '''SELECT \"Ticker\", \"Broker\" from reversed_table'''\n",
    "    reversed_ticker_df = pd.read_sql(reversed_query, conn)\n",
    "\n",
    "    tj_reversed_ticker = pd.concat([tj_ticker_df, reversed_ticker_df]).drop_duplicates()\n",
    "\n",
    "    # Fetch from EW_CONV\n",
    "    # BUY\n",
    "    ew_query_buy = '''SELECT \"Ticker\", \"Last Alert Date\", \"Buy Price\"\n",
    "    FROM ew_conv_table WHERE \"Target\" = 'NaN' '''\n",
    "    ew_conv_buy_df = pd.read_sql(ew_query_buy, conn)\n",
    "\n",
    "    last_two_dates_buy = ew_conv_buy_df['Last Alert Date'].nlargest(2)\n",
    "    ew_conv_buy_df = ew_conv_buy_df[ew_conv_buy_df['Last Alert Date'].isin(last_two_dates_buy)]\n",
    "    merge_buy = pd.merge(ew_conv_buy_df, tj_reversed_ticker, on='Ticker', how='left')\n",
    "\n",
    "    # SELL\n",
    "    ew_query_sell = '''SELECT \"Ticker\", \"Last Alert Date\", \"Target\"\n",
    "    FROM ew_conv_table WHERE \"Buy Price\" = 'NaN' '''\n",
    "    ew_conv_sell_df = pd.read_sql(ew_query_sell, conn)\n",
    "\n",
    "    last_two_dates_sell = ew_conv_sell_df['Last Alert Date'].nlargest(2)\n",
    "    ew_conv_sell_df = ew_conv_sell_df[ew_conv_sell_df['Last Alert Date'].isin(last_two_dates_sell)]\n",
    "    merge_sell = pd.merge(ew_conv_sell_df, tj_reversed_ticker, on='Ticker', how='left')\n",
    "\n",
    "    # Write the sheets to excel\n",
    "    with pd.ExcelWriter('ew_conv_buy_and_sell_today_and_yesterday.xlsx') as writer:\n",
    "        merge_buy.to_excel(writer, sheet_name='ew_conv_buy', index=False)\n",
    "        merge_sell.to_excel(writer, sheet_name='ew_conv_sell', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General function to update a table using adjust_call_status\n",
    "def update_table_with_adjusted_status(table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Fetch all records from the specified table\n",
    "    fetch_query = f'''\n",
    "    SELECT \n",
    "        \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \n",
    "        \"Close\", \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\",\n",
    "        \"Quantity\", \"Hit Type\", \"Sell Price\", \"Broker\"\n",
    "    FROM {table_name};\n",
    "    '''\n",
    "    records_df = pd.read_sql(fetch_query, conn)\n",
    "\n",
    "    # Define the adjust_call_status logic with conditional logic for table_name\n",
    "    def adjust_call_status(row):\n",
    "        if table_name == \"reversed_table\":\n",
    "            # Logic for reversed_table\n",
    "            if pd.notnull(row['CMP']) and pd.notnull(row['ll']) and row['CMP'] < row['ll']:\n",
    "                return 'Short', 'Closed', 'ISL Hit', round(row['ll'], 2)\n",
    "            elif pd.notnull(row['CMP']) and pd.notnull(row['Target']) and row['CMP'] >= row['Target']:\n",
    "                return 'Short', 'Closed', 'Target Hit', round(row['Target'], 2)\n",
    "        else:\n",
    "            # Logic for other tables\n",
    "            if pd.notnull(row['CMP']) and pd.notnull(row['ll']) and row['CMP'] < row['ll']:\n",
    "                return 'Short', 'Closed', 'ISL Hit', round(row['ll'], 2)\n",
    "            elif pd.notnull(row['CMP']) and pd.notnull(row['Target']) and row['CMP'] <= row['Target']:\n",
    "                return 'Short', 'Closed', 'Target Hit', round(row['Target'], 2)\n",
    "        return row['Call'], row['Status'], None, None\n",
    "\n",
    "    # Apply the adjust_call_status logic\n",
    "    records_df[['Call', 'Status', 'Hit Type', 'Sell Price']] = records_df.apply(\n",
    "        lambda row: adjust_call_status(row), axis=1, result_type='expand'\n",
    "    )\n",
    "    records_df['Sell Price'] = records_df['Sell Price'].round(2)\n",
    "\n",
    "    # Update the table with adjusted values\n",
    "    for _, row in records_df.iterrows():\n",
    "        update_query = f'''\n",
    "        UPDATE {table_name}\n",
    "        SET \n",
    "            \"Call\" = %s,\n",
    "            \"Status\" = %s,\n",
    "            \"Hit Type\" = %s,\n",
    "            \"Sell Price\" = %s\n",
    "        WHERE \"Buy Date\" = %s AND \"Ticker\" = %s;\n",
    "        '''\n",
    "        cur.execute(update_query, (\n",
    "            row[\"Call\"], row[\"Status\"], row[\"Hit Type\"], row[\"Sell Price\"],\n",
    "            row[\"Buy Date\"], row[\"Ticker\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "    cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for specific tables\n",
    "def update_trade_journal():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    update_table_with_adjusted_status('trade_journal', conn)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_reversed_table():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        password=\"St1uv9ac29!\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Step 1: Update CMP and CMP_Date in reversed_table from yfinance_data\n",
    "    update_cmp_query = '''\n",
    "    UPDATE reversed_table\n",
    "    SET\n",
    "        \"CMP\" = y.\"CMP\",\n",
    "        \"CMP_Date\" = y.\"CMP_Date\"\n",
    "    FROM yfinance_data y\n",
    "    WHERE reversed_table.\"Ticker\" = y.\"Ticker\";\n",
    "    '''\n",
    "    cur.execute(update_cmp_query)\n",
    "    conn.commit()\n",
    "    print(\"Updated CMP and CMP_Date in reversed_table.\")\n",
    "\n",
    "    # Step 2: Fetch updated reversed_table data\n",
    "    fetch_reversed_table_query = '''\n",
    "    SELECT \n",
    "        \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \"Close\",\n",
    "        \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\", \"Quantity\", \"Hit Type\",\n",
    "        \"Sell Price\", \"Broker\"\n",
    "    FROM reversed_table;\n",
    "    '''\n",
    "    reversed_table_df = pd.read_sql(fetch_reversed_table_query, conn)\n",
    "\n",
    "    # Step 3: Apply adjust_call_status logic\n",
    "    def adjust_call_status(row):\n",
    "        if pd.notnull(row['CMP']) and pd.notnull(row['ll']) and row['CMP'] < row['ll']:\n",
    "            return 'Short', 'Closed', 'ISL Hit', round(row['ll'], 2)\n",
    "        elif pd.notnull(row['CMP']) and pd.notnull(row['Target']) and row['CMP'] >= row['Target']:\n",
    "            return 'Short', 'Closed', 'Target Hit', round(row['Target'], 2)\n",
    "        return row['Call'], row['Status'], None, None\n",
    "\n",
    "    reversed_table_df[['Call', 'Status', 'Hit Type', 'Sell Price']] = reversed_table_df.apply(\n",
    "        lambda row: adjust_call_status(row), axis=1, result_type='expand'\n",
    "    )\n",
    "    reversed_table_df['Sell Price'] = reversed_table_df['Sell Price'].round(2)\n",
    "\n",
    "    # Step 4: Update reversed_table with adjusted values\n",
    "    for _, row in reversed_table_df.iterrows():\n",
    "        update_adjusted_query = '''\n",
    "        UPDATE reversed_table\n",
    "        SET \n",
    "            \"Call\" = %s,\n",
    "            \"Status\" = %s,\n",
    "            \"Hit Type\" = %s,\n",
    "            \"Sell Price\" = %s\n",
    "        WHERE \"Buy Date\" = %s AND \"Ticker\" = %s;\n",
    "        '''\n",
    "        cur.execute(update_adjusted_query, (\n",
    "            row[\"Call\"], row[\"Status\"], row[\"Hit Type\"], row[\"Sell Price\"],\n",
    "            row[\"Buy Date\"], row[\"Ticker\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "    print(\"Updated Call, Status, Hit Type, and Sell Price in reversed_table.\")\n",
    "\n",
    "    # Step 5: Archive Closed Records from reversed_table\n",
    "    to_archive = reversed_table_df[reversed_table_df[\"Status\"] == \"Closed\"]\n",
    "    for _, row in to_archive.iterrows():\n",
    "        insert_into_archive_query = '''\n",
    "        INSERT INTO archived_trades (\n",
    "            \"Buy Date\", \"Ticker\", \"Buy Strat\", \"Last Alert Date\", \"ll\", \"Close\", \n",
    "            \"Target\", \"Call\", \"Status\", \"CMP\", \"CMP_Date\", \"Quantity\", \"Hit Type\", \n",
    "            \"Sell Price\", \"Broker\"\n",
    "        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (\"Buy Date\", \"Ticker\")\n",
    "        DO NOTHING;\n",
    "        '''\n",
    "        cur.execute(insert_into_archive_query, (\n",
    "            row[\"Buy Date\"], row[\"Ticker\"], row[\"Buy Strat\"], row[\"Last Alert Date\"],\n",
    "            row[\"ll\"], row[\"Close\"], row[\"Target\"], row[\"Call\"], row[\"Status\"],\n",
    "            row[\"CMP\"], row[\"CMP_Date\"], row[\"Quantity\"], row[\"Hit Type\"],\n",
    "            row[\"Sell Price\"], row[\"Broker\"]\n",
    "        ))\n",
    "    conn.commit()\n",
    "    print(\"Archived closed records from reversed_table.\")\n",
    "\n",
    "    # Step 6: Delete Closed Records from reversed_table\n",
    "    delete_query = '''\n",
    "    DELETE FROM reversed_table\n",
    "    WHERE \"Status\" = 'Closed';\n",
    "    '''\n",
    "    cur.execute(delete_query)\n",
    "    conn.commit()\n",
    "    print(\"Deleted closed records from reversed_table.\")\n",
    "\n",
    "    cur.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\1673951837.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  cmp_df = pd.read_sql(cmp_query, conn)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\1673951837.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  ew_conv_df = pd.read_sql(ew_conv_query, conn)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\1673951837.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  merged_tickers_df = pd.read_sql(merged_tickers_query, conn)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\2235367292.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  records_df = pd.read_sql(fetch_query, conn)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\3918243691.py:42: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  buy_price_df = pd.read_sql(buy_price_query, conn)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\3918243691.py:56: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  closed_records_df = pd.read_sql(closed_records_query, conn)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_18628\\3992719907.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  records_df = pd.read_sql(fetch_records_query, conn)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     fetch_and_export_to_excel()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m update_trade_journal()\n\u001b[0;32m      7\u001b[0m transfer_closed()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtransfer_reversed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m update_reversed_table()\n\u001b[0;32m     10\u001b[0m fetch_and_export_to_excel()\n",
      "Cell \u001b[1;32mIn[8], line 85\u001b[0m, in \u001b[0;36mtransfer_reversed\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCall\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Apply the logic to update the DataFrame\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43mrecords_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStatus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHit Type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSell Price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m records_df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: adjust_call_status(row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Round Sell Price to 2 decimal places\u001b[39;00m\n\u001b[0;32m     90\u001b[0m records_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSell Price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m records_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSell Price\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\frame.py:4341\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4337\u001b[0m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[0;32m   4338\u001b[0m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[0;32m   4340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m-> 4341\u001b[0m         \u001b[43mcheck_key_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m   4343\u001b[0m             \u001b[38;5;28mself\u001b[39m[k1] \u001b[38;5;241m=\u001b[39m value[k2]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:390\u001b[0m, in \u001b[0;36mcheck_key_length\u001b[1;34m(columns, key, value)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "\u001b[1;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "# Main function to run the entire pipeline\n",
    "def main():\n",
    "    final_table = load_and_merge_csv()\n",
    "    merged_table = fetch_and_merge_from_postgresql(final_table)\n",
    "    save_and_upload(merged_table)\n",
    "    update_trade_journal()\n",
    "    transfer_closed()\n",
    "    transfer_reversed()\n",
    "    update_reversed_table()\n",
    "    fetch_and_export_to_excel()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
