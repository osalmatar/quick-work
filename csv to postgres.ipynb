{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting CSV to PostgreSQL with Data Transformations\n",
    "This notebook demonstrates how to load data from a CSV file, apply transformations based on your observations, and export it to a PostgreSQL database.\n",
    "\n",
    "## Key Observations and Updates:\n",
    "1. **NaN Values**: Keep `NaN` for categorical columns and replace missing values with `None` for numerical columns.\n",
    "2. **Binary Transformation**: \n",
    "   - `Alert`: Binary (0 or 1).\n",
    "   - `BearBreak`: Transform \"Bearish Breakout\" to `1` and empty values to `0`.\n",
    "3. **Date/Time Format**: Convert `Date/Time` to a proper `datetime` format.\n",
    "4. **Ensure Data Types**: Ensure consistency in data types across columns (e.g., `Alert` as float, `BearBreak` as binary, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Function to explore dataset\n",
    "# You can uncomment this to explore the dataset before applying transformations\n",
    "\n",
    "# def explore_dataset(df):\n",
    "#     \"\"\"\n",
    "#     Function to show unique values, data types, and NaN counts for each column in the DataFrame.\n",
    "#     \"\"\"\n",
    "#     for column in df.columns:\n",
    "#         unique_values = df[column].unique()\n",
    "#         num_unique = len(unique_values)\n",
    "#         num_missing = df[column].isna().sum()\n",
    "#         dtype = df[column].dtype\n",
    "\n",
    "#         # Display the column's name, data type, and number of unique values\n",
    "#         print(f\"\\nColumn: {column}\")\n",
    "#         print(f\"Data Type: {dtype}\")\n",
    "#         print(f\"Number of Unique Values: {num_unique}\")\n",
    "#         print(f\"Number of Missing Values: {num_missing}\")\n",
    "#         print(f\"Unique Values: {unique_values[:10]}\")  # Display the first 10 unique values only for readability\n",
    "\n",
    "#         # If there are more than 10 unique values, just print a note\n",
    "#         if num_unique > 10:\n",
    "#             print(f\"... and {num_unique - 10} more unique values\")\n",
    "\n",
    "# Load DataFrame from CSV and specify column types\n",
    "def load_from_csv():\n",
    "    \"\"\"\n",
    "    Load CSV and specify column types for certain fields.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        'merged_tickers.csv',\n",
    "        dtype={\n",
    "            'Alert': 'float', \n",
    "            'BearBreak': 'str', \n",
    "            'Buy_ATR': 'str',\n",
    "            'Buy_BB': 'str', \n",
    "            'Buy_EW': 'str', \n",
    "            'Buy_LR': 'str', \n",
    "            'Buy_ZZ': 'str',\n",
    "            'Pattern': 'str'\n",
    "        },\n",
    "        low_memory=False\n",
    "    )\n",
    "\n",
    "# Prepare DataFrame for insertion by converting Date/Time, replacing NaN, and ensuring correct data types\n",
    "def prepare_data_for_insert(df):\n",
    "    \"\"\"\n",
    "    Prepare data by applying transformations:\n",
    "    1. Convert Date/Time to datetime.\n",
    "    2. Replace NaN with None for numerical values and keep NaN for categorical columns.\n",
    "    3. Binary conversion for 'Alert' and 'BearBreak'.\n",
    "    4. Ensure proper data types for all columns.\n",
    "    \"\"\"\n",
    "    # Convert 'Date/Time' to datetime format\n",
    "    df['Date/Time'] = pd.to_datetime(df['Date/Time'], errors='coerce')\n",
    "\n",
    "    # Replace NaN with None for numerical columns while keeping NaN for categorical columns\n",
    "    df = df.where(pd.notna(df), None)\n",
    "    \n",
    "    # Handle binary conversion\n",
    "    df['Alert'] = df['Alert'].astype(float, errors='ignore')  # Ensure 'Alert' is binary (0 or 1)\n",
    "    \n",
    "    # Convert 'BearBreak' to binary: 1 for 'Bearish Breakout' and 0 for others\n",
    "    df['BearBreak'] = df['BearBreak'].apply(lambda x: 1 if x == 'Bearish Breakout' else 0)\n",
    "\n",
    "    # Ensure the correct types for other columns\n",
    "    df['Buy_ATR'] = df['Buy_ATR'].astype(str, errors='ignore')\n",
    "    df['Buy_BB'] = df['Buy_BB'].astype(str, errors='ignore')\n",
    "    df['Buy_EW'] = df['Buy_EW'].astype(str, errors='ignore')\n",
    "    df['Buy_LR'] = df['Buy_LR'].astype(str, errors='ignore')\n",
    "    df['Buy_ZZ'] = df['Buy_ZZ'].astype(str, errors='ignore')\n",
    "    df['Pattern'] = df['Pattern'].astype(str, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to insert DataFrame into PostgreSQL\n",
    "def insert_to_postgres(df):\n",
    "    \"\"\"\n",
    "    Connect to PostgreSQL and insert the DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to PostgreSQL\n",
    "        conn = psycopg2.connect(\n",
    "            host=\"localhost\",  # your host\n",
    "            database=\"postgres\",  # your database\n",
    "            user=\"postgres\",  # your username\n",
    "            password=\"your_password\",  # replace with your PostgreSQL password\n",
    "            port=\"5432\"  # default port for PostgreSQL\n",
    "        )\n",
    "        \n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Create table in PostgreSQL if it doesn't exist\n",
    "        create_table_query = '''\n",
    "        CREATE TABLE IF NOT EXISTS joined_tickers (\n",
    "            ticker TEXT,\n",
    "            date_time TIMESTAMP,\n",
    "            buy TEXT,\n",
    "            alert NUMERIC,\n",
    "            bearbreak NUMERIC,  -- Binary (1 for Bearish Breakout, 0 otherwise)\n",
    "            buy_atr TEXT,\n",
    "            buy_bb TEXT,\n",
    "            buy_ew TEXT,\n",
    "            buy_lr TEXT,\n",
    "            buy_zz TEXT,\n",
    "            close NUMERIC,\n",
    "            dnbars NUMERIC,\n",
    "            downtrend_length NUMERIC,\n",
    "            fromhigh NUMERIC,\n",
    "            fromlow NUMERIC,\n",
    "            lr_lc NUMERIC,\n",
    "            lr_mc NUMERIC,\n",
    "            lr_uc NUMERIC,\n",
    "            mega_sellby_larger_wave NUMERIC,\n",
    "            numberoftickers NUMERIC,\n",
    "            pattern TEXT,\n",
    "            percentilerank NUMERIC,\n",
    "            rank NUMERIC,\n",
    "            s1 NUMERIC,\n",
    "            sctr NUMERIC,\n",
    "            sl NUMERIC,\n",
    "            upbars NUMERIC,\n",
    "            uptrend_length NUMERIC,\n",
    "            hh NUMERIC,\n",
    "            ll NUMERIC,\n",
    "            myshort NUMERIC\n",
    "        );\n",
    "        '''\n",
    "        cur.execute(create_table_query)\n",
    "        conn.commit()\n",
    "\n",
    "        # Insert data into PostgreSQL\n",
    "        insert_query = '''\n",
    "        INSERT INTO joined_tickers (ticker, date_time, buy, alert, bearbreak, buy_atr, buy_bb, buy_ew, buy_lr, buy_zz, close, \n",
    "            dnbars, downtrend_length, fromhigh, fromlow, lr_lc, lr_mc, lr_uc, mega_sellby_larger_wave, numberoftickers, \n",
    "            pattern, percentilerank, rank, s1, sctr, sl, upbars, uptrend_length, hh, ll, myshort)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        '''\n",
    "\n",
    "        # Iterate over DataFrame and insert each row\n",
    "        for _, row in df.iterrows():\n",
    "            values = (\n",
    "                row['Ticker'], \n",
    "                row['Date/Time'], \n",
    "                row['Buy'], \n",
    "                row.get('Alert', None),\n",
    "                row.get('BearBreak', None),\n",
    "                row.get('Buy_ATR', None),\n",
    "                row.get('Buy_BB', None),\n",
    "                row.get('Buy_EW', None),\n",
    "                row.get('Buy_LR', None),\n",
    "                row.get('Buy_ZZ', None),\n",
    "                row.get('Close', None),\n",
    "                row.get('DnBars', None),\n",
    "                row.get('Downtrend Length', None),\n",
    "                row.get('FromHigh', None),\n",
    "                row.get('FromLow', None),\n",
    "                row.get('LR_LC', None),\n",
    "                row.get('LR_MC', None),\n",
    "                row.get('LR_UC', None),\n",
    "                row.get('Mega Sellby Larger Wave', None),\n",
    "                row.get('NumberOfTickers', None),\n",
    "                row.get('Pattern', None),\n",
    "                row.get('PercentileRank', None),\n",
    "                row.get('Rank', None),\n",
    "                row.get('S1', None),\n",
    "                row.get('SCTR', None),\n",
    "                row.get('SL', None),\n",
    "                row.get('UpBars', None),\n",
    "                row.get('Uptrend Length', None),\n",
    "                row.get('hh', None),\n",
    "                row.get('ll', None),\n",
    "                row.get('myShort', None)\n",
    "            )\n",
    "\n",
    "            cur.execute(insert_query, values)\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        # Verify data insertion\n",
    "        cur.execute(\"SELECT * FROM joined_tickers LIMIT 10\")\n",
    "        rows = cur.fetchall()\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "\n",
    "        # Close the cursor and connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as error:\n",
    "        print(f\"Error: {error}\")\n",
    "\n",
    "# Load and prepare the data\n",
    "df = load_from_csv()\n",
    "\n",
    "# Uncomment the next line if you want to explore the dataset before transformation\n",
    "# explore_dataset(df)\n",
    "\n",
    "df = prepare_data_for_insert(df)\n",
    "\n",
    "# Insert the DataFrame into PostgreSQL\n",
    "insert_to_postgres(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "1. **Run the code** in this notebook to load, transform, and export the data into PostgreSQL.\n",
    "2. **Verify the Data**: After exporting, query the PostgreSQL database to verify that the transformations were correctly applied and the data is properly inserted.\n",
    "3. **Explore Dataset**: If you want to explore the dataset before applying transformations, uncomment the `explore_dataset(df)` line.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
